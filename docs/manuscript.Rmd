---
title: "A Monte Carlo study on methods for handling class imbalance"
author: "Mark H. White II | markhwhiteii@gmail.com"
fontsize: 12pt
geometry: margin=1in
output: pdf_document
---

```{r warning = FALSE, message = FALSE, results = FALSE, echo = FALSE}
library(tidyverse)
library(ggridges)
library(knitr)
options(knitr.kable.NA = "")
library(rstan)
library(rstanarm)
dat <- read_csv("../data/results_df.csv") %>% 
  bind_rows(read_csv("../data/results_df_101-150.csv")) %>% 
  bind_rows(read_csv("../data/results_df_151-200.csv")) %>% 
  bind_rows(read_csv("../data/results_df_201-300.csv")) %>% 
  bind_rows(read_csv("../data/results_df_301-350.csv")) %>% 
  bind_rows(read_csv("../data/results_df_351-450.csv")) %>% 
  bind_rows(read_csv("../data/results_df_451-500.csv")) %>% 
  separate(v1, c("algorithm", "sampling"), "_", FALSE) %>% 
  mutate(
    v1 = as.factor(v1),
    algorithm = factor(
      algorithm, levels = c("c50", "adaboost", "randomforest", "xgboost")
      ),
    sampling = as.factor(sampling),
    predict_vars = linear_vars + 5,
    iter = as.factor(iter),
    spec = tn / (tn + fp),
    auc_roc = (1 + rec - (1 - spec)) / 2
  )
names(dat)[1] <- "model"
levels(dat$model) <- c(
  "AdaBoost, None", "AdaBoost, Over", "AdaBoost, SMOTE", "AdaBoost, Under",
  "C5.0, None", "C5.0, Over", "C5.0, SMOTE", "C5.0, Under",
  "Random Forest, None", "Random Forest, Over", 
  "Random Forest, SMOTE", "Random Forest, Under",
  "XGBoost, None", "XGBoost, Over", "XGBoost, SMOTE", "XGBoost, Under")
dat <- dat[, !names(dat) %in% c("cor_vars", "linear_vars")]
```

Many applications of classification problems in machine learning involve class imbalance—a situation where the class of interest (the "minority" or "positive" class) makes up a very small percentage of the total data (i.e., 5% or less of cases are in this class). Algorithms that try to maximize the overall accuracy of prediction bias in favor of the majority (or "negative") class. For example, if 1% of cases are in the positive class, then an algorithm can ensure 99% accuracy by simply predicting *all* cases as "negative." This obviously misses the whole point of training a model in the first place, as often we are training the model explicitly because we want to be able to predict this class or event with a low base rate. Class imbalance problems arise in important applications: Will a contact lead to a donation to our campaign? Does this person have cancer? Is this credit card transaction fraudulent?  

There are a number of ways to address class imbalance in two-class classification problems (Branco, Torgo, & Ribeiro, 2015; Longadge, Dongre, & Malik, 2013). When researchers propose new methods or review a number of them, they generally use real-world, preexisting data sets to see how their methods perform (Galar, Fernández, Barrenechea, Bustince, & Herrera, 2012; Weiss, McCarthy, & Zabar, 2007). While this adds the benefit of ecological validity, it does not allow us to see systematic relationships between characteristics of the data and performance. Monte Carlo simulation methods can be used to generate sets of data that are identical in every way, save for characteristics we are interested in studying. Because these simulation methods control aspects of the data while holding everything else constant, simulation studies can yield conclusions like, "Algorithm X outperforms Algorithm Y, but only in small samples." We would know that this is due to the sample size (and not some other aspect of the data), because we can control characteristics of the data when setting the parameters to our simulations.  

The present paper takes a Monte Carlo approach, systematically varying certain aspects about the data across 500 simulations (while holding other aspects constant).  I will employ all combinations of four algorithmic approaches and four sampling approaches to addressing class imbalance. I will create one data generating process that only allows the following aspects to vary: sample size, amount of class imbalance, number of predictors, and proportion of predictors that are noise (i.e., do not predict the outcome).  

# Method

## Data Generating Process

I generated data with a dichotomous outcome. The average class imbalance was 97% in the negative and 3% in the positive class. These data were simulated by adapting the `twoClassSim` function from the `caret` R package (Kuhn, 2008). I generated 500 data sets using the following steps:  

1. The number of cases in each data set were randomly drawn from a distribution, $N(40000, 5000)$.  

2. Two multivariate normal predictors ($A$ and $B$) were generated, correlating with one another at $r = .65$. These two variables contributed to the log-odds by: $4A + 4B + 2AB$.  

3. Another variable, $J \sim U(-1, 1)$, was generated. This variable further added to the log-odds by: $J^3 + 2 \times \exp(-6 \times (J - 0.3)^2)$.  

4. Two more variables, $K \sim U(0, 1)$ and $L \sim U(0, 1)$, were generated and contributed to the log-odds by: $2 \times \sin(K \times L)$.  

5. A number $X$ was drawn from a distribution, $N(50, 7)$. Another number $Y$ was drawn from a distribution, $N(.15, .033)$. $Z = X - (X \times Y)$ variables were drawn from a distribution, $N(0, 1)$. Each of these $Z$ variables further added to the log-odds in an additive fashion, where coefficients were (a) of alternating signs and (b) evenly spaced from $2.50$ to $0.25$.  

6. $Y \over 2$ variables were drawn from a distribution, $N(0, 1)$, and did not contribute to the log-odds. However, they were still used as inputs in each of the models.  

7. The log-odds for each case were converted to probabilities. For each data set, a positive (i.e., minority) class proportion, $M$, was sampled from a distribution, $N(.03, .007)$. Probabilities were sorted from lowest to highest. The difference between the probability for the $1 - M$th highest probability and $M$ was calculated, and this constant was added to the probability for each case. This was done to ensure the minority class proportion was about $M$.  

## Models

For each of the 500 data sets, I randomly divided the data into training and testing sets along a $.70 / .30$ split. I trained 16 models on the training cases for each data set. These models were created by every iteration of 4 sampling techniques and 4 algorithms. I discuss each in turn.  

### Sampling Techniques

Data were preprocessed using four different techniques (using the `ubUnder`, `ubOver`, and `ubSMOTE` functions from the `unbalanced` R package, respectively; Pozzolo, Caelen, & Bontempi, 2015):  

- **Undersampling.** Cases were randomly dropped from the majority (i.e., negative) class until it was the same size as the minority (i.e., positive) class. For example, if there were $i$ cases in the minority and $j$ cases in the majority class, $j - i$ cases were randomly discarded from the majority class.  

- **Oversampling.** Cases were randomly replicated, with replacement, from the minority class until it was the same size as the majority class. For example, if there were $i$ cases in the minority and $j$ cases in the majority class, $j - i$ random replications, with replacement, were made of cases in the minority class.  

- **Synthetic minority over-sampling technique (SMOTE).** "Synthetic" cases were made from the minority class as a way to "oversample" it, while certain cases from the majority class were randomly dropped from the data set. There are many variants of SMOTE, but I employed the version described by Chawla, Bowyer, Hall, & Kegelmeyer (2002). Synthetic cases were created along the following process: for each case in the minority class, (a) find its $k$ nearest neighbors, (b) randomly selecting 2 of these nearest neighbors, (c) calculate a line connecting the original case to each of these two randomly-selected nearest neighbors, (d) choosing a random point on these lines, (e) saving these points as a new case, and (d) labeling these new cases as part of the minority class. For each synthetically-generated minority case, 2 of the cases from the majority class were included in the "SMOTEd" data set. For example, if an original data set contained 90 cases in the negative class and 10 in the positive class, the "SMOTEd" data set would include 40 negative cases and 30 positive cases.  

- **None.** This was a control condition, and no preprocessing was done to the data. The class imbalance was not adjusted for in the preparation of the data.  

### Algorithms

Predictions were made using four different algorithms:  

- **Random forest.** I employed Breiman's (2001) random forest approach using the `randomForest` function from the R package of the same name (Liaw & Wiener, 2002). This involves training $t$ number of decision trees on a reduced data set with a subset of $p$ predictors and $n$ resampled—with replacement—cases from the inputted data set. Predictions are made by letting each of these $t$ trees predict the outcome for any given case, and a simple majority vote is taken on how to classify the case. For example, if 80 trees predicted "A" and 20 trees predicted "B," the case would be predicted as class "A." In the event of a tie, the prediction is randomly made. For the current study, I set $t = 100$, $p$ equal to the square root of the number of predictors in the inputted data set, and $n$ equal to the number of cases in the inputted data set. I did not limit how many nodes each tree was allowed to form, and the minimum number of cases required to create a node was set at one. Given `X` as the inputs and `y` as the class label, the code for the random forest was: `randomForest(X, y, ntree = 100)`.  

- **AdaBoost.** I employed Freund and Schapire's (1996) AdaBoost.M1 approach using the `adaboost` function from the `fastAdaboost` R package (Chatterjee, 2016). This algorithm also involves training $t$ number of decision trees (note that this method can be used with any learning algorithm, but the function used here employs decision trees). Instead of generating a number of trees independently of one another based on a subset of cases and predictors (like the random forest), AdaBoost works in serial. Once the the first tree ($t = 1$) is fit, cases in the second tree ($t = 2$) are weighted based on the error of the first tree. The third tree $t = 3$, in turn, is trained on cases weighted by the error of the second tree. That is, the weights for each decision tree $t$ are determined by the error of $t - 1$. In the case of the first tree, every case has the same weight, $1 \over n$, where $n$ is the sample size of the data set. Weights are calculated such that cases that $t - 1$ predicts *incorrectly* are weighted *more* in $t$. In this way, AdaBoost focuses in on the "mistakes" of the previous trees. AdaBoost labels one class -1 and the other +1, and the final prediction is based on taking the sign of the weighted sum of predictions and their weights: $\text{sign}(\alpha_1h_1(x) + ... + \alpha_th_t(x))$ where $t$ is the total number of trees, $\alpha$ are weights, and $h_t(x)$ are predictions from a tree. For the current study, I set $t = 10$. Given `y` as the class label and `data` as the name of the data frame, the code for AdaBoost was: `adaboost(y ~ ., data, nIter = 10)`.  

- **Gradient boosting.** I employed the other most popular form of boosting, gradient boosting, as well. I used the "extreme gradient boosting," or "XGBoost" variant (Chen & Guestrin, 2016) of gradient boosting, using the `xgboost` function from the R package of the same name (Chen, He, Benesty, Khotilovich, & Yang, 2017). While AdaBoost learns from the "mistakes" of previous trees (again, other classifiers can be used here, but decision trees are employed in the current study) by calculating weights for the cases, gradient boosting learns from the "mistakes" by training trees $t$ that are trained on the residuals of $t - 1$. Residuals are treated as negative gradients, and the model is updated with new trees $t$ using gradient descent. XGBoost is one of the most popular implementations, optimizing gradient boosting in both terms of efficiency and accuracy. Like with AdaBoost, I set the number of trees to fit at $t = 10$; the rest of the hyperparameters were left to the package defaults. The code for XGBoost was: `xgboost(X, y, nrounds = 10, objective = "binary:logistic")`.  

- **Decision tree.** I also employed one decision tree as a "control" non-ensemble learner. I used Ross Quinlan's C5.0 algorithm (see Kuhn & Johnson, 2013 for a description) using the `C5.0` function from the `C50` R package (Kuhn, Weston, Coulter, & Culp, 2015). The code for this decision tree was: `C5.0(X, y)`.  

## Performance Assessment

These 16 models then made predictions on the holdout cases. For each model, the confusion matrix was recorded and a number of performance metrics were used to compare the models.  

The overall accuracy of a learner is not a useful metric for assessing performance in the presence of class imbalance. For example, if the positive class is only 2% of the data set, then we could achieve 98% accuracy simply by labeling every class in the negative. This is essentially useless, given that the whole reason of undertaking the machine learning task in this case is to be able to predict the positive class. For this reason, I will focus on four other metrics:  

- **Recall.** The proportion of positive cases that were predicted correctly. This is the number of true positives (TP) over the sum of TP and false negatives (FN), $TP \over TP + FN$. This quantifies what proportion of the cases we were interested in predicting correctly were actually recovered by our model.  

- **Precision.** The proportion of *correct* positive predictions. This is TP over the sum of TP and false positives (FP), $TP \over TP + FP$. This quantifies what proportion of the cases we predicted as a positive class were *actually* of the positive class. This is also known as "positive predictive value," because it measures how "valuable" a positive prediction is.  

- **F1 score.** This is the harmonic mean of recall and precision, $F_1 = 2 \times {\text{precision} \times \text{recall} \over \text{precision} + \text{recall}}$. It is a weighted mean where both recall and precision contribute to the score equally. It is one way to assess model performance when classes are imbalanced (Wallace, Small, Brodley, & Trikalinos, 2011), as it balances the benefit of predicting a minority class correctly with the cost of predicting a majority class incorrectly. Scores range from 0 (worst) to 1 (perfect).  

```{r echo = FALSE, out.width = "65%", fig.align = "center", fig.cap = "An example of an ROC plot showing AUC. Random guessing is denoted by the dotted line, where AUC is 0.5."}
roc <- data.frame(x = c(0, 1, 0, .4, 1),
                  y = c(0, 1, 0, .6, 1),
                  z = c(1, 1, 0, 0, 0))
ggplot(roc, aes(x = x, y = y, linetype = factor(z))) +
  geom_line() +
  geom_point(aes(x = .4, y = .6)) +
  geom_point(aes(x = .5, y = .5)) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(x = "False Positive Rate", y = "Recall") +
  annotate("text", x = .35, y = .70, label = "AUC = .60 \n (.40, .60)")
```
  
- **AUC(ROC).** This is short for the "area under the receiver operating characteristic curve." In the present analyses, each model will have a curve determined by one point in two-dimensional space. The model's false-positive rate, $FP \over FP + TN$, is plotted on the x-axis, while the recall is plotted on the y-axis. One point is plotted for each model. A straight line then connects the point to the bottom-left and top-right corners of the figure. The area underneath this curve is the AUC. It is calculated as $1 + R - FPR \over 2$, were $R$ is the recall and $FPR$ is the false-positive rate. I used this as my primary measure of model performance, following Galar et al.'s (2012) review. See Figure 1 for an example.  

# Results

The metrics I want to use to compare various approaches require that *some* positive predictions are made. However, some models might simply label all (or nearly all) cases in the negative to achieve, on average, 97% accuracy. Table 1 shows what proportion of the time (out of the 500 simulations) that the combination of sampling technique and algorithm made no positive class predictions.  

```{r echo = FALSE}
prop_zeros <- dat %>% 
  mutate(total_pos = tp + fp) %>% 
  group_by(model) %>% 
  summarise_at("total_pos", funs(mean(. == 0))) %>% 
  arrange(desc(total_pos))
kable(prop_zeros, col.names = c("Model", "Proportion P = 0"), 
      caption = "Proportion of simulations where each model made zero positive class predictions.")
```
  
First, this illustrates the relative importance of sampling techniques over the algorithm used. Oversampling and doing no data preprocessing often led models to simply predict every case in the negative class. However, making at least *one* positive prediction is an incredibly low bar. Remember that the denominator of the $F_1$ score is precision $+$ recall. If the model does not make enough positive predictions to make this sum $> 0$, the $F_1$ score cannot be calculated. Table 2 shows what proportion of the time the combination of sampling technique and algorithm yielded so few positive predictions that an $F_1$ score was undefined.  

```{r echo = FALSE}
prop_f1_nan <- dat %>% 
  mutate(f1_nan = is.nan(f1)) %>% 
  group_by(model) %>% 
  summarise_at("f1_nan", funs(mean(. == TRUE))) %>% 
  arrange(desc(f1_nan))
kable(prop_f1_nan, col.names = c("Model", "Proportion F1 is N/A"), 
      caption = "Proportion of simulations where each model made so few positive class predictions that an F1 score could not be calculated.")
```
  
Again, the sampling techniques discriminated on this proportion more than did the algorithms employed. All of the oversampling and no preprocessing models had at least one instance where the $F_1$ score was undefined, Although C5.0 and XGBoost, both using oversampling, were much closer to 0 than to 1, for the rest of the analyses, I focus on the models using SMOTE and undersampling alone.  

## Comparing Mean Model Performance

Tabe 3 lists the means for each metric on the models employing SMOTE and undersampling as data preprocessing techniques. These models are sorted in decreasing fashion from best AUC to worst AUC. What discriminates these models the most is the effect of using undersampling versus SMOTE on recall: The highest recall using undersampling was $.676$, while the best using SMOTE was $.335$.  

```{r echo = FALSE}
good_models <- prop_f1_nan %>% 
  filter(f1_nan == 0) %>% 
  pull(model) %>% 
  as.character()
mean_results <- dat %>% 
  filter(model %in% good_models) %>% 
  select(model, prec, rec, f1, auc_roc) %>% 
  group_by(model) %>% 
  summarise_if(is.numeric, mean) %>% 
  arrange(desc(auc_roc))
kable(mean_results, digits = 3, col.names = 
        c("Model", "Precision", "Recall", "F1", "AUC(ROC)"),
      caption = "Mean performance on each metric for models using SMOTE and undersampling.")
```
  
It is not only important to get good performance on average, however; the variance should also be examined. Figure 2 displays density plots from the 500 simulations for each of the 8 models using SMOTE and undersampling and each of the 4 metrics. While using a single C5.0 decision tree employed after undersampling appeared to separate itself above the models using SMOTE in terms of recall, it is clear that this model displays far more variance than the rest of the models using undersampling (as shown by the wider-tailed density than the ensemble methods).  

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Density plots for all 4 performance metrics for each of the 8 models using SMOTE and undersampling.", out.width = "75%", fig.align = "center"}
good_dat <- dat[dat$model %in% good_models, ]
ranked_names <- names(sort(tapply(
  good_dat$auc_roc, droplevels(good_dat$model), mean, na.rm = TRUE),
  decreasing = TRUE
))
good_dat$model <- factor(good_dat$model, levels = c(ranked_names))
dat_long_ridge <- good_dat %>% 
  transmute(
    model = model, 
    `AUC(ROC)` = auc_roc, 
    F1 = f1, 
    Precision = prec, 
    Recall = rec) %>% 
  gather(metric, value, -model)
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", 
               "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
ggplot(dat_long_ridge, aes(y = model, x = value, fill = model)) +
  geom_density_ridges(alpha = .7) +
  facet_wrap( ~ metric, scales = "free_x") +
  theme_minimal() +
  labs(x = NULL, y = "Model") +
  theme(text = element_text(size = 14), legend.position = "none") +
  scale_fill_manual(values = cbPalette)
```
  
Table 3 and Figure 2 show that, on the primary metric of interest (AUC), the ensemble methods (i.e., random forest, XGBoost, AdaBoost) that employed undersampling performed the best. I compared the means of all four metrics between these three models (i.e., Random Forest, Under; XGBoost, Under; AdaBoost, Under). To do this, I ran four separate Bayesian models, using Stan. First, I estimated the mean for each of the 8 models using SMOTE and undersampling. Then, I calculated pairwise differences between each of the 3 undersampled ensemble models. The priors for each of the 8 means were normally-distributed, with a mean that was equal to the grand mean of all 8 models on the relevant metric. The standard deviations for the priors of group means were $.10$ for recall and AUC, while they were $.005$ for precision and $F_1$ scores. The prior for the error was an uninformative half-cauchy prior with a mean of 0 and standard deviation of 1. Results for these comparisons are shown in Table 4.  

```{r warning = FALSE, message = FALSE, results = FALSE, echo = FALSE}
stan_code <- "
data {
  int n; // observations
  int k; // number of models
  real gm; // grand mean
  real psigmu; // prior for mu_sigma
  int x[n]; // group indicator
  vector[n] y; // outcome
}
parameters {
  real alpha[k]; // model means
  real<lower=0> sigma; // error
}
transformed parameters {
  real rf_xg_diff; // compare rf and xg
  real rf_ada_diff; // compare rf and ada
  real xg_ada_diff; // compare xg and ada
  rf_xg_diff = alpha[1] - alpha[2]; // compute diff
  rf_ada_diff = alpha[1] - alpha[3]; // compute diff
  xg_ada_diff = alpha[2] - alpha[3]; // compute diff
}
model {
  vector[n] mu; // expected value
  for (i in 1:n) { // for all people
    mu[i] = alpha[x[i]]; //their mu is their group's mean
  }
  y ~ normal(mu, sigma); // likelihood
  alpha ~ normal(gm, psigmu); // prior, defined from data
  sigma ~ cauchy(0, 1); // uninformative prior on sigma
}
"
dat_long <- good_dat %>% 
  mutate(model = as.numeric(model)) %>% 
  select(model, auc_roc, f1, prec, rec)
set.seed(1839)
sdat_aucroc <- list(n = nrow(dat_long), k = length(unique(dat_long$model)), 
                    gm = mean(dat_long$auc_roc), psigmu = .10, x = dat_long$model,
                    y = dat_long$auc_roc)
sdat_f1 <- list(n = nrow(dat_long), k = length(unique(dat_long$model)), 
                    gm = mean(dat_long$f1), psigmu = .005, x = dat_long$model,
                    y = dat_long$f1)
sdat_prec <- list(n = nrow(dat_long), k = length(unique(dat_long$model)), 
                    gm = mean(dat_long$prec), psigmu = .005, x = dat_long$model,
                    y = dat_long$prec)
sdat_rec <- list(n = nrow(dat_long), k = length(unique(dat_long$model)), 
                    gm = mean(dat_long$rec), psigmu = .10, x = dat_long$model,
                    y = dat_long$rec)
mdiff_aucroc <- stan(model_code = stan_code, data = sdat_aucroc,
                     control = list(max_treedepth = 15), cores = 4)
mdiff_f1 <- stan(model_code = stan_code, data = sdat_f1,
                 control = list(max_treedepth = 15), cores = 4)
mdiff_prec <- stan(model_code = stan_code, data = sdat_prec,
                   control = list(max_treedepth = 15), cores = 4)
mdiff_rec <- stan(model_code = stan_code, data = sdat_rec,
                  control = list(max_treedepth = 15), cores = 4)
mean_diffs <- round(summary(mdiff_aucroc)$summary[10:12, c(1, 4, 8)], 3) %>% 
  rbind(round(summary(mdiff_f1)$summary[10:12, c(1, 4, 8)], 3)) %>% 
  rbind(round(summary(mdiff_rec)$summary[10:12, c(1, 4, 8)], 3)) %>% 
  rbind(round(summary(mdiff_prec)$summary[10:12, c(1, 4, 8)], 3)) %>% 
  as.data.frame() %>% 
  rownames_to_column("Pairwise Comparison") %>% 
  mutate(`Pairwise Comparison` = rep(c(
        "Random Forest - XGBoost", 
        "Random Forest - AdaBoost",
        "XGBoost - AdaBoost"), 4),
         Outcome = c("AUC(ROC)", "", "", "F1", "", "",
                     "Recall", "", "", "Precision", "", ""))
mean_diffs <- mean_diffs[, c(5, 1, 2:4)]
colnames(mean_diffs)[3] <- "Difference"
```
``` {r echo = FALSE}
kable(mean_diffs, caption = "Pairwise mean differences and 95% credible intervals between all three ensemble models using undersampling.")
```
  
First, it is important to remember that these models are only those using undersampling. Random forest outperformed the other two ensembles on every metric, except for recall. A random forest was able to cover 1.9% and 3.0% more AUC than XGBoost and AdaBoost, respectively. Considering recall, AdaBoost recovered 6.9% and 9.1% more of the positive cases than the random forest and XGBoost models, respectively.  

## Data Characteristics and Performance

Lastly, I looked at the relationship between characteristics of the data (i.e., minority size, sample size, noise variable, and predictors) and the performance of the model in terms of AUC. The bivariate relationships between the data characteristics and the AUC, split by model, are presented in Figure 3. The primary interest here was to see if there were any interactions between data characteristics and model; however, we can see that no meaningful interactions are present: Even though data characteristics were predictors of performance, the main effect of model held across the range of minority size, sample size, and number of noise and predictor variables.  
  
```{r echo = FALSE, fig.cap = "Relationships between data characteristics and AUC, by model.", out.width = "75%", fig.align = "center"}
auc_cor_dat <- dat[dat$model %in% good_models, ] %>% 
  transmute(Model = model, `AUC(ROC)` = auc_roc, N = n,  
            `Minority Size` = minority_size, Predictors = predict_vars,
            `Noise Variables` = noise_vars) %>% 
  gather("key", "value", 3:6)
ggplot(auc_cor_dat, 
       aes(x = value, y = `AUC(ROC)`, group = Model, color = Model)) +
  geom_point(alpha = .4) +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~ key, scales = "free_x") +
  labs(x = NULL) +
  theme_minimal() +
  scale_colour_manual(values = cbPalette)
```
  
Curiously, it appears that *more* input variables led to *worse* performance. Since the data generation procedure defined noise variables as a proportion of predictor variables, the two are positively-related. A better model, then, is to collapse across models and predict AUC from all four characteristics of the data in one model. I did this using a Bayesian multiple regression model in Stan. Uninformative priors, $N(0, 10)$, were set for all regression coefficients. A half-cauchy prior with a mean of 0 and standard deviation of 10 was set for the error term. As can be seen in Table 5, less class imbalance led to greater AUC.  

```{r warning = FALSE, message = FALSE, results = FALSE, echo = FALSE}
mod_characs <- stan_glm(auc_roc ~ n + noise_vars + minority_size + predict_vars,
                        data = good_dat, prior_intercept = normal(0, 10),
                        prior = normal(0, 10), prior_aux = cauchy(0, 10))
```
``` {r echo = FALSE}
mod_characs <- as.data.frame(summary(mod_characs))[1:5, c(1, 4, 8)]
colnames(mod_characs)[1] <- "Coefficient"
rownames(mod_characs)[2:5] <- c("N", "Noise Variables", "Minority Size",
                                "Predictor Variables")
kable(mod_characs, digits = 3, caption = "Regression coefficients and 95% credible intervals predicting AUC(ROC).")
```

# Discussion

Preprocessing techniques affected model performance in situations of class imbalance more than what algorithm was employed. The best models used a combination of undersampling and an ensemble method. Overall, the best performing model was involved undersampling the data and then using a random forest. Additionally, controlling the properties of the data using Monte Carlo simulation methods allows us to investigate relationships between data characteristics and model performance, while holding all other properties of the data constant. Sample size, minority size, number of predictors, and number of noise variables were the only elements of the data that varied between each of the 500 simulations. No interactions between model and characteristics of the data were found; the model that performed the best on averaged tended to perform the best regardless of any given characteristic of the data. The only relationship found was that greater class imbalance led to worse performance (again, regardless of model used).  

However, it must be noted that these relationships might not hold when properties of the data reach far beyond the parameters set here. More simulation studies should vary the ranges of these data characteristics further. Additionally, different data-generating procedures could be employed to generalize across different types of data structures (e.g., non-linear relationships between predictor variables, different relationships between predictors and the outcome).  

The comparions used here were based on rather agnostic models. In practice, one likely knows the relative costs of false positives versus the benefits of true positives. This tradeoff could be worked directly into a model using a cost matrix or selecting a model using a variant of the $F$ score (which can incorporate the relative importance of precision and recall) to measure performance. Additionally, I undertook no hyperparameter tuning based on cross-validation in the current study. This was primarily done for the sake of computational effort. However, proponents of their favorite method here might argue that—if they could tune the model—then the results here might look different. They might be correct, but the primary focus here was to (a) compare a number of common approaches as they appear out-of-the-box and in accordance with rules-of-thumb and (b) demonstrate the usefulness of Monte Carlo methods in making conclusions like "Oversampling appears to the the best for small data sets" (p. 147; KrishnaVeni & Rani, 2011), instead of relying on comparing preexisting data sets that might differ in any other number of unknown ways.  

# References

Branco, P., Torgo, L., & Ribeiro, R. (2015). A survey of predictive modelling under imbalanced distributions. Retrieved from https://arxiv.org/abs/1505.01658.  

Breiman, L. (2001). Random forests. *Machine Learning, 45*, 5-32.  

Chatterjee, S. (2016). *fastAdaboost: A Fast Implementation of Adaboost*. R package version 1.0.0. https://CRAN.R-project.org/package=fastAdaboost.  

Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, W. (2002). SMOTE: Synthetic minority over-sampling technique. *Journal of Artificial Intelligence Research, 16*, 321-357.  

Chen, T. & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Retrieved from https://arxiv.org/abs/1603.02754.  

Chen, T., He, T., Benesty, M., Khotilovich, V., & Tang, Y. (2017). *xgboost: Extreme Gradient Boosting.* R package version 0.6-4. https://CRAN.R-project.org/package=xgboost.  

Freund, Y. & Schapire, R. E. (1996). Experiments with a new boosting algorithm. *Machine Learning: Proceedings of the Thirteenth International Conference.*  

Galar, M., Fernández, A., Barrenechea, E., Bustince, H., & Herrera, F. (2012). A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches. *IEE Transactions on Systems, Man, and Cybernatics—Part C: Applications and Reviews, 42*(4), 463-484.  

KrishnaVeni, C. V. & Rani, T. S. (2011). On the classification of imbalanced datasets. *International Journal of Computing Science & Technology, 2*(1). 145-148.  

Kuhn, M. (2008). *caret: Classification and Regression Training.* R package version 6.0-77. https://CRAN.R-project.org/package=caret.  

Kuhn, M. & Johnson, K. (2013). *Applied Predictive Modeling*. New York, NY: Springer.  

Kuhn, M., Weston, S., Coulter, N., & Culp, M. (2015). *C50: C5.0 Decision Trees and Rule-Based Models.* R package version 0.1.0-24. https://CRAN.R-project.org/package=C50.  

Liaw, A. & Wiener, M. (2002). Classification and Regression by randomForest. *R News, 2*(3), 18-22.  

Longadge, R., Dongre, S., & Malik, L. (2013). Class imbalance problem in data mining: Review. Retrieved from https://arxiv.org/abs/1305.1707.  

Pozzolo, A. D., Caelen, O., & Bontempi, G. (2015). *unbalanced: Racing and Unbalanced Methods Selection.* R package version 2.0. https://CRAN.R-project.org/package=unbalanced.  

Wallace, B. C., Small, K., Brodley, C. E., & Trikalinos, T. A. (2011). Class imbalance, redux. *11th IEEE International Conference on Data Mining*, 754-763.  

Weiss, G. M., McCarthy, K., & Zabar, B. (2007). Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs? *DMIN, 7*, 35-41.  
