---
title: "A Monte Carlo study on methods for handling class imbalance"
author: "Mark H. White II | markhwhiteii@gmail.com"
output: pdf_document
---

# Method

## Data Generating Process

Two class data were simulated by adapting the `twoClassSim` function from the `caret` R package (Kuhn, 2008):  

- Two multivariate normal predictors ($A$ and $B$) are generated. $A$ and $B$ are correlated at $r = .65$. These two variables contributed to the log-odds by $4A + 4B + 2AB$.  

- Another variable, $J \sim U(-1, 1)$, was generated. This variable further added to the log-odds by $J^3 + 2 \times \exp(-6 \times (J - 0.3)^2)$.  

- Two more variables, $K \sim U(0, 1)$ and $L \sim U(0, 1)$, were generated and contributed to the log-odds by $2 \times \sin(K \times L)$.  

- For each data set, a number $X$ was selected, where $X \sim N(50, 7)$. Another number, $Y$, was selected, where $Y \sim N(.15, .033)$. $Z = X - (X \times Y)$ variables were generated from a $N(0, 1)$ distribution. Each of these $Z$ variables further added to the log-odds in a simple additive fashion, where coefficients were (a) of alternating signs and (b) evenly spaced from $2.50$ to $0.25$.  

- $Y \over 2$ variables were generated from a $N(0, 1)$ distribution and did not contribute to the log-odds.  

- The log-odds for each case were converted to probabilities. For each data set, a positive (i.e., minority) class proportion, $M$, was sampled from $N(.03, .007)$. Probabilities were sorted from lowest to highest. The difference between the probability for the $1 - M$th highest probability and $M$ was calculated, and this constant was added to the probability for each case.  

- Lastly, the number of cases in each data set were randomly drawn from a distribution $N(40000, 5000)$.  500 data sets were generated, and sixteen combinations of sampling techniques and algorithms were fit to each of these data sets.  

## Sampling Techniques

Data were preprocessed using four different techniques (using the `ubUnder`, `ubOver`, and `ubSMOTE` functions from the `unbalanced` R package, respectively; CITE, XXXX):  

- **Undersampling.** Cases were randomly dropped from the majority (i.e., negative) class until it was the same size as the minority (i.e., positive) class. For example, if there were $i$ cases in the minority and $j$ cases in the majority class, $j - i$ cases were randomly discarded from the majority class.  

- **Oversampling.** Cases were randomly replicated, with replacement, from the minority class until it was the same size as the majority class. For example, if there were $i$ cases in the minority and $j$ cases in the majority class, $j - i$ random replications, with replacement, were made of cases in the minority class.  

- **Synthetic minority over-sampling technique (SMOTE).** "Synthetic" cases are made from the minority class as a way to "oversample" it, while certain cases from the majority class are randomly dropped from the data set. There are many variants of SMOTE, but I employed the version described by Chawla, Bowyer, Hall, & Kegelmeyer (2002). Synthetic cases were created by (a) finding the $k$ nearest neighbors, with $k$ set to 5, for each case from the minority class, (b) randomly selecting 2 of these nearest neighbors for each case, (c) calculate a line connecting the original case to each of these two randomly-selected nearest neighbors, (d) choosing a random point on this line and saving it as a new case, and (e) labeling this new case part of the minority class. For each synthetically-generated minority case, 2 of the cases from the majority class were included in the "SMOTEd" data set. For example, if an original data set contained 90 cases in the negative class and 10 in the positive class, the "SMOTEd" data set would include 40 negative cases and 30 positive cases.  

- **None.** This was a control condition, and no preprocessing was done to the data. The class imbalance was not adjusted for in the preparation of the data.  

## Algorithms

Predictions were made using four different algorithms:  

- **Random forest.** I employed Breiman's (2001) random forest approach using the `randomForest` function from the R package of the same name (CITE, XXXX). This involves training $t$ number of decision trees on a reduced data set with a subset of $p$ predictors and $n$ resampled—with replacement—cases from the inputted data set. Predictions are made by letting each of these $t$ trees predict the outcome for any given case, and a simple majority vote is taken on how to classify the case. For example, if 80 trees predicted "A" and 20 trees predicted "B," the case would be predicted as class "A." In the event of a tie, the prediction is randomly made. For the current study, I set $t = 100$, $p$ equal to the square root of the number of predictors in the inputted data set, and $n$ equal to the number of cases in the inputted data set. I did not limit how many nodes each tree was allowed to form, and the minimum number of cases required to create a node was set at one. Given `X` as the inputs and `y` as the class label, the code for the random forest was:  

``` {r eval = FALSE}
randomForest(X, y, ntree = 100)
```

- **AdaBoost.** I employed Freund and Schapire's (1996) AdaBoost.M1 approach using the `adaboost` function from the `fastAdaboost` R package (CITE, XXXX). This algorithm also involves training $t$ number of decision trees (the method can be used with any learning algorithm, but the function used here employs decision trees). Instead of generating a number of trees independently of one another based on a subset of cases and predictors (like the random forest), AdaBoost works in serial. Once the the first tree ($t = 1$) is fit, cases in the second tree ($t = 2$) are weighted based on the error of the first tree. The third tree $t = 3$, in turn, is trained on cases weighted by the error of the second tree. That is, the weights for each decision tree $t$ are determined by the error of $t - 1$. In the case of the first tree, every case has the same weight, $1 \over n$, where $n$ is the sample size of the data set. Weights are calculated such that cases that $t - 1$ predicts *incorrectly* are weighted *more* in $t$. In this way, AdaBoost focuses in on the "mistakes" of the previous trees. AdaBoost labels one class -1 and the other +1, and the final prediction is based on taking the sign of the weighted sum of predictions and their weights: sign($\alpha_1h_1(x) + ... + \alpha_th_t(x)$) where $t$ is the total number of trees, $\alpha$ are weights, and $h_t(x)$ are predictions from a tree. For the current study, I set $t = 10$. Given `y` as the class label and `data` as the name of the data frame, the code for AdaBoost was:  

``` {r eval = FALSE}
adaboost(y ~ ., data, nIter = 10)
```

- **Gradient boosting.** I employed the other most popular form of boosting, gradient boosting, as well. I used the "extreme gradient boosting," or "XGBoost" variant (CITE, XXXX) of gradient boosting, using the `xgboost` function from the R package of the same name (CITE, XXXX). While AdaBoost learns from the "mistakes" of previous trees (again, other classifiers can be used here, but decision trees are employed in the current study) by calculating weights for the cases, gradient boosting learns from the "mistakes" by training trees $t$ that are trained on the residuals of $t - 1$. Residuals are treated as negative gradients, and the model is updated with new trees $t$ using gradient descent. XGBoost is one of the most popular implementations, providing a number of optimizations of gradient boosting in both terms of efficiency and accuracy (CITE, XXXX). Like with AdaBoost, I set the number of trees to fit at $t = 10$; the rest of the hyperparameters were left to the package defaults. The code for XGBoost was:  

```{r eval = FALSE}
xgboost(X, y, nrounds = 10, verbose = 0, objective = "binary:logistic")
```

- **Decision tree.** I also employed one decision tree as a "control," non-ensemble learner. I used Ross Quinlan's C5.0 algorithm (see CITE, XXXX for a description) using the `C5.0` function from the `C50` R package (CITE , XXXX). The code for this decision tree was:  

```{r eval = FALSE}
C5.0(X, y)
```

## Performance Assessment



# Results


